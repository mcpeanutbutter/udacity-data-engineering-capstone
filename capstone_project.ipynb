{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DWH augmentation for US immigration data\n",
    "\n",
    "\n",
    "## Project Summary\n",
    "In this project, we transform US immigration data for usage in a data warehouse. We also enrich this data with auxilliary information about airports and weather data. \n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "What is _not_ covered here is the actual upload of the data into a suitable database and the execution of test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scope of the project\n",
    "\n",
    "The following data sets were provided by Udacity:\n",
    "* I94 Immigration Data: Immigration data from the US National Tourism and Trade Office in parquet file format. A data dictionary is included.\n",
    "* World Temperature Data: A temperature dataset from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "* U.S. City Demographic Data: A US demographic dataset from [OpenSoft](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "* Airport Code Table: A table of airport codes and corresponding cities from [datahub](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "The goal is to model this data and provide connections between the data fields that can be used by a data scientist who works with a DWH. This means that the relevant data from the temperature, the demographic and the airport data sets should be linked to the immigration data, which acts as the main fact table. \n",
    "\n",
    "We will use PySpark for the main fact table (immigration data) and Pandas for the dimension tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sas = spark.read.parquet(\"sas_data\")\n",
    "df_airport = pd.read_csv(\"data/airport-codes_csv.csv\")\n",
    "df_demo = pd.read_csv(\"data/us-cities-demographics.csv\", sep=';')\n",
    "df_weather = pd.read_csv(\"temperature_data/GlobalLandTemperaturesByCountry.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating additional data from the SAS dictionary\n",
    "\n",
    "Here, we extract tables from the SAS dictionary using a custom function. we will also perform some basic cleaning steps on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/I94_SAS_Labels_Descriptions.SAS') as file:\n",
    "    \n",
    "    def clean_field(df, col, regex):\n",
    "        '''Extracts regex from dataframe column, removes whitespaces and converts to upper case.'''\n",
    "        df[col] = df[col].str.extract(regex)\n",
    "        df[col] = df[col].str.strip()\n",
    "        df[col] = df[col].str.upper()\n",
    "        return df[col]\n",
    "    \n",
    "    lines=file.readlines()\n",
    "    \n",
    "    df_cntyl = pd.DataFrame(lines[9:297])\n",
    "    df_cntyl = df_cntyl[0].str.split(\"=\", n=1, expand= True)\n",
    "    df_cntyl.columns = ['i94cntyl','country']\n",
    "    df_cntyl['country'] = clean_field(df_cntyl, 'country', r'\\'([^\\']+)\\'')\n",
    "    df_cntyl['i94cntyl'] = df_cntyl['i94cntyl'].astype(int)\n",
    "    \n",
    "    df_port = pd.DataFrame(lines[302:962])\n",
    "    df_port = df_port[0].str.split(\"=\", n=1, expand= True)\n",
    "    df_port_comma_split = df_port[1].str.split(\",\", n=1, expand= True)\n",
    "    df_port[1] = df_port_comma_split[0]\n",
    "    df_port[2] = df_port_comma_split[1]\n",
    "    df_port.columns = ['i94port','port','addr']\n",
    "    df_port['i94port'] = clean_field(df_port, 'i94port', r'\\'([^\\']+)\\'')\n",
    "    df_port['port'] = clean_field(df_port, 'port', r'\\'([^\\']+)')\n",
    "    df_port['addr'] = clean_field(df_port, 'addr', r'([^\\']+)\\'')\n",
    "  \n",
    "    df_mode = pd.DataFrame(lines[972:976])\n",
    "    df_mode = df_mode[0].str.split(\"=\", n=1, expand= True)\n",
    "    df_mode.columns = ['i94mode','mode']\n",
    "    df_mode['mode'] = clean_field(df_mode, 'mode', r'\\'([^\\']+)\\'')\n",
    "    df_mode['i94mode'] = clean_field(df_mode, 'i94mode', r'\\s+([^\\']+)')\n",
    "    \n",
    "    df_addr = pd.DataFrame(lines[981:1036])\n",
    "    df_addr = df_addr[0].str.split(\"=\", n=1, expand= True)\n",
    "    df_addr.columns = ['i94addr','state']\n",
    "    df_addr['i94addr'] = clean_field(df_addr, 'i94addr', r'\\'([^\\']+)\\'')\n",
    "    df_addr['state'] = clean_field(df_addr, 'state', r'\\'([^\\']+)\\'')\n",
    "    \n",
    "    df_visa = pd.DataFrame(lines[1046:1049])\n",
    "    df_visa = df_visa[0].str.split(\"=\", n=1, expand= True)\n",
    "    df_visa.columns = ['i94visa','visa']\n",
    "    df_visa['visa'] = clean_field(df_visa, 'visa', r'([^\\']+)\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data exploration and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration data\n",
    "\n",
    "First, we show the schema and some rows of the immigration data to get a feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows of data\n",
    "df_sas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# schema\n",
    "df_sas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first 10 rows of data\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(df_sas.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also decide to drop fields that have a significant fraction of null values and perform some additional cleanup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the relative number of null values \n",
    "df_sas_total_rows = df_sas.count()\n",
    "df_sas_nulls = df_sas.select([(count(when(isnan(c) | col(c).isNull(), c))/df_sas_total_rows).alias(c) for c in df_sas.columns]).toPandas()\n",
    "\n",
    "# Drop columns with over 90% null values. \n",
    "# Note: This step is for demonstration purposes; in a real project I would leave\n",
    "# this decision to a data scientist.\n",
    "empty_cols = []\n",
    "for c in df_sas_nulls.columns:\n",
    "    if df_sas_nulls[c][0] > 0.9:\n",
    "        empty_cols.append(c)\n",
    "print(empty_cols)\n",
    "df_sas_clean_a = df_sas.drop(*empty_cols)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with duplicate ids\n",
    "df_sas_clean_b = df_sas_clean_a.dropna(how='all', subset=['cicid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert double columns to the original format (integer)\n",
    "df_sas_clean_c = df_sas_clean_b.\\\n",
    "withColumn(\"cicid\", df_sas_clean_b[\"cicid\"].cast('integer')).\\\n",
    "withColumn(\"i94yr\", df_sas_clean_b[\"i94yr\"].cast('integer')).\\\n",
    "withColumn(\"i94mon\", df_sas_clean_b[\"i94mon\"].cast('integer')).\\\n",
    "withColumn(\"i94cit\", df_sas_clean_b[\"i94cit\"].cast('integer')).\\\n",
    "withColumn(\"i94res\", df_sas_clean_b[\"i94res\"].cast('integer')).\\\n",
    "withColumn(\"arrdate\", df_sas_clean_b[\"arrdate\"].cast('integer')).\\\n",
    "withColumn(\"i94mode\", df_sas_clean_b[\"i94mode\"].cast('integer')).\\\n",
    "withColumn(\"i94bir\", df_sas_clean_b[\"i94bir\"].cast('integer')).\\\n",
    "withColumn(\"count\", df_sas_clean_b[\"count\"].cast('integer')).\\\n",
    "withColumn(\"i94visa\", df_sas_clean_b[\"i94visa\"].cast('integer')).\\\n",
    "withColumn(\"depdate\", df_sas_clean_b[\"depdate\"].cast('integer')).\\\n",
    "withColumn(\"biryear\", df_sas_clean_b[\"biryear\"].cast('integer')).\\\n",
    "withColumn(\"admnum\", df_sas_clean_b[\"admnum\"].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert SAS date format to datetime:\n",
    "def date_add_(days):\n",
    "    '''Converts SAS time to datetime.'''\n",
    "    date = datetime.strptime('1960-01-01', \"%Y-%m-%d\")\n",
    "    return date + timedelta(days)\n",
    "\n",
    "date_add_udf = f.udf(date_add_, t.DateType())\n",
    "\n",
    "df_sas_clean_d = df_sas_clean_c.withColumn('arrdate', date_add_udf('arrdate'))\\\n",
    "    .withColumn('depdate', date_add_udf('depdate'))\n",
    "\n",
    "# Drop year and mon columns\n",
    "df_sas_clean_e = df_sas_clean_d.drop('i94yr','i94mon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have decided against keeping year and month columns (or even generating an additional day column), since we do not actually have weather data for the given data, and hence a direct join would not make much sense even from a technical perspective. Instead, I leave it up to the data scientist on the receiving end of the data to process the date values and join them as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the weather and cyntl data, the country column is capitalized to enable joins. We also convert the weather date string to datetime format. The demographic column names have many spaces and capitalization, so we adjust them to be more DWH-friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.columns=['date','average_temperature','average_temperature_uncertainty','country']\n",
    "df_weather['country'] = df_weather['country'].str.upper().astype(str)\n",
    "df_weather['date'] = pd.to_datetime(df_weather['date'])\n",
    "df_weather=df_weather[df_weather['average_temperature'].notnull()]\n",
    "\n",
    "df_cntyl['country'] = df_cntyl['country'].str.upper().astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.columns=['city', 'state', 'median_age', 'male_population', 'female_population',\n",
    "       'total_population', 'number_of_veterans', 'foreign_born',\n",
    "       'average_household_size', 'state_code', 'race', 'count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Defining the Data Model\n",
    "\n",
    "Three major steps will be necessary to relate the selected data:\n",
    "\n",
    "### Connecting the weather data\n",
    "\n",
    "Here, we will have to add the (somewhat obscure) i94 country code to the weather data, such that it can be used by the dwh users for a join. We can also remove countries from the data that do not appear in the immigration data\n",
    "\n",
    "### Connecting the demographic data\n",
    "\n",
    "For the demographic data, a state code is already available, which can be used for joining the data to the immigration data. However, the demographic data is listed by city (as opposed to state). Hence, we will need to perform aggregations on a state level. \n",
    "\n",
    "### Connecting the airport data\n",
    "\n",
    "In this case, it will be necessary to map the airport codes used in the immigration data to the airport codes in the airport data.\n",
    "\n",
    "### Data sources from the SAS data dictionary\n",
    "\n",
    "These sources can mostly be used as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "\n",
    "Here, we build the data pipelines to create the data model. We also define a testing function to perform some basic data quality checks on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_quality_check(df):\n",
    "    '''Generates description of dataframe argument.'''\n",
    "    \n",
    "    if(df.index.is_unique):\n",
    "        print(\"The dataframe has a unique index.\")\n",
    "    else:\n",
    "        print(\"Warning: The dataframe does not have a unique index.\")\n",
    "    \n",
    "    col_summary = dict()\n",
    "    for c in df.columns:\n",
    "        col_attributes = dict()\n",
    "        col_attributes['dtype'] = df[c].dtype\n",
    "        col_attributes['count'] = df[c].count()\n",
    "        col_attributes['count_null'] = df[c].size - col_attributes['count']\n",
    "        col_attributes['unique_values'] = df[c].nunique()\n",
    "    \n",
    "        col_summary[c] = col_attributes\n",
    "    return pd.DataFrame(col_summary).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration data\n",
    "\n",
    "With the cleanups we already did, the immigration data should actually be fine as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_immigration_dwh = df_sas_clean_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather data\n",
    "\n",
    "In order to be able to join the weather data to the immigration data, the cntyl country code needs to be available in the weather data. This is done via a join on the country field. Ideally, this join would be fuzzy, but for now we will just perform a rigid join.\n",
    "\n",
    "We leave the actual aggreagation of weather data over time to the data scientist. This implies that country names will have multiple appearances, and hence cannot be used as an index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_dwh = pd.merge(left=df_weather, right=df_cntyl, \n",
    "                          left_on='country', right_on='country',\n",
    "                          how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather_dwh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep countries that are in the imigration data\n",
    "i94cntyl_in_sas = list(set(df_immigration_dwh.select(\"i94cit\").distinct().toPandas()['i94cit'] \\\n",
    "+ df_immigration_dwh.select(\"i94res\").distinct().toPandas()['i94res']))\n",
    "i94cntyl_in_sas = [int(x) for x in i94cntyl_in_sas if str(x) != 'nan']\n",
    "\n",
    "df_weather_dwh = df_weather_dwh[df_weather_dwh['i94cntyl'].notnull()]\n",
    "df_weather_dwh = df_weather_dwh[df_weather_dwh['i94cntyl'].isin(i94cntyl_in_sas)]\n",
    "df_weather_dwh = df_weather_dwh.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_quality_check(df_weather_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic data\n",
    "We aggregate the available numeric data on a city level for each state. Since we don't have access to total state demographics in this data set, we express the male population, female population, veteran number and foreign born number as fractions of total pupolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_dwh = df_demo[['state_code', 'state']].drop_duplicates().set_index('state_code')\\\n",
    ".join(df_demo.groupby(['state_code'])['male_population', 'female_population',\\\n",
    "                                      'total_population', 'number_of_veterans', 'foreign_born'].agg('sum'))\\\n",
    ".join(df_demo.groupby(['state_code'])['median_age', 'average_household_size'].agg('median'))\n",
    "\n",
    "df_demo_dwh['male_population'] = df_demo_dwh['male_population']/df_demo_dwh['total_population']\n",
    "df_demo_dwh['female_population'] = df_demo_dwh['female_population']/df_demo_dwh['total_population']\n",
    "df_demo_dwh['number_of_veterans'] = df_demo_dwh['number_of_veterans']/df_demo_dwh['total_population']\n",
    "df_demo_dwh['foreign_born'] = df_demo_dwh['foreign_born']/df_demo_dwh['total_population']\n",
    "\n",
    "df_demo_dwh = df_demo_dwh.drop(['total_population'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_demo_dwh.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_quality_check(df_demo_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the data above, we can also extract the \"race distribution\" of each state in a similar fashion. This table acts as an additinal dimension table for each state code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_race_dwh = pd.DataFrame(df_demo.groupby(['state_code', 'race'])['count'].agg('sum'))\\\n",
    ".join(df_demo.groupby(['state_code'])['count'].agg('sum'), rsuffix='_total')\n",
    "\n",
    "df_demo_race_dwh['fraction']=df_demo_race_dwh['count']/df_demo_race_dwh['count_total']\n",
    "df_demo_race_dwh = pd.DataFrame(df_demo_race_dwh['fraction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo_race_dwh.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_quality_check(df_demo_race_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we extracted df_port from the sas data file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_port.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to combine this information with the available airport information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_dwh = pd.merge(left=df_port, right=df_airport, \n",
    "                          left_on='i94port', right_on='ident',\n",
    "                          how='left')\n",
    "df_airport_dwh = df_airport_dwh.drop(['ident'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The join is acctually successful in some occasions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_airport_dwh[df_airport_dwh['type'].notnull()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_quality_check(df_airport_dwh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use an additional check: The iso_region field from df_airport should match with the addr field from df_port. Let us check the cases where this is _not_ true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_dwh['iso_region_state'] = clean_field(df_airport_dwh, 'iso_region', r'-([^-]+)')\n",
    "df_airport_dwh_outliers = df_airport_dwh[\\\n",
    "    (df_airport_dwh['type'].notnull())\\\n",
    "    & (df_airport_dwh['iso_region_state'] != df_airport_dwh['addr'])]\n",
    "\n",
    "print(len(df_airport_dwh_outliers))\n",
    "df_airport_dwh_outliers.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these are not even in the US, which clearly indicates a false join. This makes it hard to trust the data we generated with the join. We should _at least_ remove these cases, even though they will leave very little data to work with. I will leave this as an option question an simple flag the data with the improper join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport_dwh['false_join'] = df_airport_dwh.index.isin(df_airport_dwh_outliers.index.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from SAS dictionary\n",
    "\n",
    "#### Mode data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mode data can be taken as-is with the correct index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mode_dwh=df_mode.set_index('i94mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mode_dwh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visa data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visa data can be taken as-is with the correct index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa_dwh = df_visa.set_index('i94visa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_visa_dwh.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data only contains information which is already available. Hence, it will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_addr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dictionary\n",
    "Below, for each field we provide a what the data is and where it came from. \n",
    "\n",
    "*Note:* The **bold** values link to other fact/dimension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_immigration_dwh\n",
    "\n",
    "This is the fact table with the immigration data. \n",
    "\n",
    "| field | type | description | origin |\n",
    "| --- | --- | --- | --- |\n",
    "| cicid | int | unique id | original parquet/sas files\n",
    "| **i94cit** | int | country code (birth) | original parquet/sas files\n",
    "| **i94res** | int | country code (residence) | original parquet/sas files\n",
    "| **i94port** | string | arrival airport | original parquet/sas files\n",
    "| arrdate | date | arrival date | original parquet/sas files\n",
    "| **i94mode** | int | mode of transportation | original parquet/sas files\n",
    "| **i94addr** | string | arrival state code | original parquet/sas files\n",
    "| depdate | int | departure date | original parquet/sas files\n",
    "| i94bir | int | age of respondent in years | original parquet/sas files\n",
    "| **i94visa** | int | visa type | original parquet/sas files\n",
    "| count | int | summary statistics | original parquet/sas files\n",
    "| dtadfile | string | character date field | original parquet/sas files\n",
    "| visapost | string | Department of State where where Visa was issued | original parquet/sas files\n",
    "| entdepa | string | Arrival Flag | original parquet/sas files\n",
    "| entdepd | string | Departure Flag | original parquet/sas files\n",
    "| matflag | string | Match flag | original parquet/sas files\n",
    "| biryear | int | 4 digit year of birth | original parquet/sas files\n",
    "| dtaddto | string | character date field | original parquet/sas files\n",
    "| gender | string | Non-immigrant sex | original parquet/sas files\n",
    "| airline | string | Airline used to arrive in US | original parquet/sas files\n",
    "| admnum | int | Admission number | original parquet/sas files\n",
    "| fltno | string | Flight number | original parquet/sas files\n",
    "| visatype | string | class of admission | original parquet/sas files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_weather_dwh\n",
    "\n",
    "This is a fact table with weather data by country and date. \n",
    "\n",
    "| field | type | description | origin |\n",
    "| --- | --- | --- | --- |\n",
    "| index | int | unique id | generated |\n",
    "| date | date | date of record | weather data |\n",
    "| average_temperature | numeric |average temperature |weather data |\n",
    "| average_temperature_uncertainty |  numeric | average temperature uncertainty  | weather data |\n",
    "| country | string | country | weather data |\n",
    "| **i94cntyl** | int | cntyl country code | SAS description via join | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_demo_dwh\n",
    "\n",
    "This is a dimension table that provides demographic data for the states in the us.\n",
    "\n",
    "| field | type | description | origin |\n",
    "| --- | --- | --- | --- |\n",
    "| **state_code** | string | us state code | demographic data |\n",
    "| state | string | us state | demographic data |\n",
    "| male_population | numeric | fraction of males |demographic data |\n",
    "| female_population | numeric | fraction of females |demographic data |\n",
    "| number_of_veterans | numeric | fraction of veterans | demographic data |\n",
    "| foreign_born | numeric | fraction of foreign borns | demographic data |\n",
    "| median_age | numeric | median age | demographic data |\n",
    "| average_household_size | numeric | average household size | demographic data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_demo_race_dwh\n",
    "\n",
    "This is a dimension table that provides racial data for the states in the us. \n",
    "\n",
    "| field | type | description | origin |\n",
    "| --- | --- | --- | --- |\n",
    "| **state_code** | string | us state code | demographic data | \n",
    "| race | string | race (forms unique index combined with state_code) | demographic data | \n",
    "| fraction | numeric | fration of race | demographic data | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_mode_dwh\n",
    "\n",
    "This is a dimension table that provides the full form of the transportation mode.\n",
    "\n",
    "| field | type | description | origin |\n",
    "| --- | --- | --- | --- |\n",
    "| **i94mode** | int | mode of transportation (numeric) | SAS description | \n",
    "| mode | string | mode of transportation | SAS description | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_visa_dwh\n",
    "\n",
    "This is a dimension table that provides the full form of the visa type.\n",
    "\n",
    "| field | type | description | origin |\n",
    "| --- | --- | --- | --- |\n",
    "| **i94visa** | int | visa type (numeric) | SAS description | \n",
    "| visa | string | visa type | SAS description | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_airport_dwh\n",
    "\n",
    "This is a dimension table that provides detailed information for airports.\n",
    "\n",
    "| field | type | description | origin |\n",
    "| --- | --- | --- | --- |\n",
    "| i94port | string | airport code | SAS description |\n",
    "| port | string | short airport name | SAS description |\n",
    "| addr | string | us state code | SAS description |\n",
    "| type | string | airport type | airport data |\n",
    "| name | string | full airport name | airport data |\n",
    "| elevation_ft | numeric | airport elevation | airport data |\n",
    "| continent | string | continent | airport data |\n",
    "| iso_country | string | country of airport | airport data |\n",
    "| iso_region | string | region of airport | airport data |\n",
    "| municipality | string | municipality of airport | airport data |\n",
    "| gps_code | string | airport gps code | airport data |\n",
    "| iata_code | string | airport iata code | airport data |\n",
    "| local_code | string | airport local code | airport data |\n",
    "| coordinates | string | airport gps coordinates | airport data |\n",
    "| false_join | bool | indicates if airport data might be faulty | airport data |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Project Summary\n",
    "\n",
    "\n",
    "### Outline\n",
    "\n",
    "In this project, we enriched US immigration data with dimension data from three other data sets:\n",
    "* Temperature data of origin countries\n",
    "* US state demographic data\n",
    "* Airport data\n",
    "\n",
    "The data can be joined by DWH users via specific columns in the immigration data. \n",
    "\n",
    "We did not cover the actual upload of data to a suitable DWH. At this stage of the project, the data is available in PySpark and Pandas dataframes. \n",
    "\n",
    "### Choice of tools\n",
    "The immigration data contains over 3 million rows. With its scalable big-data capabilities, (Py)Spark is a natural choice to perform the data wrangling in such a scenario. I decided to use Pandas for the remaining data sets, since they could quickly be loaded as Pandas dataframes and Pandas syntax is slighlty more consice (compared with Spark). \n",
    "\n",
    "To simplify the development, all steps are performed in a Jupyter Notebook. Of course, in a production environment, one should use more robust and version-friendly tools, such as a proper Python project with .py scripts.\n",
    "\n",
    "\n",
    "### Data updates\n",
    "The data used in the project is static and hence there were no considerations of regular updates. If there was a desire for regular updates, a proper automatic data pipeline would need to be established - this is covered below. \n",
    "\n",
    "\n",
    "### Outlook\n",
    "\n",
    "We briefly describe how this project would need to be adapted in three different scenarios.\n",
    "\n",
    "#### Scenario 1: Increase in data volume by 100x\n",
    "If the immigration data increased by a factor of 100x, Spark is still the proper tool to use. In order to perform the cleaning steps above more efficiently, it might be required to use an actual spark cluster and distribute the computation to multiple nodes.\n",
    "\n",
    "For the data used for dimensions, we do not expect them to increase by 100x, since they are already covering the whole world or at least all US states. However, in the unlikely case that this data will eventually run over memory, it will be necessary to migrate the computations from Pandas to PySpark. \n",
    "\n",
    "\n",
    "#### Scenario 2: The data populates a dashboard that must be updated on a daily basis by 7am every day\n",
    "In this scenario a transition to a data orchestration platform, such as Apache Airflow, is required. This is needed to reliably perform ETL pipelines and report any issues along the way. Alternatively, one could also just run this script with as a CRON job (provided updated input data), but a dedicated tool is clearly the better choice.\n",
    "\n",
    "Additionally, one should consider adjusting the code to allow for incremental updates. \n",
    "\n",
    "#### Scenario 3: The database needs to be accessed by 100+ people\n",
    "If access by many people is required, the data should be made available on a scalable DWH platform, such as AWS Redhift. In this case, the easiest solution would be to  \n",
    "1. Move the data to S3 buckets in a suitable format (e.g. csv)\n",
    "2. Load the data from S3 to Redhift with an ETL pipeline extension (ideally via Airflow or similar). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
